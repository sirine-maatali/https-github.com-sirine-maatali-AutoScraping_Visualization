{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job links extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 0 existing links.\n",
      "la localisation est : France\n",
      "New links saved to 'Feuille 3'.\n",
      "All links saved to 'Feuille 2'.\n"
     ]
    }
   ],
   "source": [
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from gspread_dataframe import get_as_dataframe, set_with_dataframe\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import time\n",
    "\n",
    "# Define scope and credentials\n",
    "scope = ['https://www.googleapis.com/auth/spreadsheets', \"https://www.googleapis.com/auth/drive.file\", \"https://www.googleapis.com/auth/drive\"]\n",
    "Cred = ServiceAccountCredentials.from_json_keyfile_name('credentials.json', scope)\n",
    "client = gspread.authorize(Cred)\n",
    "\n",
    "\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "def extract_location(url):\n",
    "    # Parse the URL\n",
    "    parsed_url = urlparse(url)\n",
    "    \n",
    "    # Extract the query parameters\n",
    "    query_params = parse_qs(parsed_url.query)\n",
    "    \n",
    "    # Get the location parameter (after 'l=')\n",
    "    location = query_params.get('l', [None])[0]\n",
    "    print(\"la localisation est :\",location)\n",
    "    return location\n",
    "\n",
    "\n",
    "\n",
    "# Open or create the spreadsheet\n",
    "spreadsheet_name = 'try'\n",
    "spreadsheet = None\n",
    "try:\n",
    "    spreadsheet = client.open(spreadsheet_name)\n",
    "except gspread.exceptions.SpreadsheetNotFound:\n",
    "    spreadsheet = client.create(spreadsheet_name)\n",
    "\n",
    "# Get or create sheets\n",
    "try:\n",
    "    sheet2 = spreadsheet.worksheet(\"Feuille 2\")\n",
    "except gspread.exceptions.WorksheetNotFound:\n",
    "    sheet2 = spreadsheet.add_worksheet(title=\"Feuille 2\", rows=\"1000\", cols=\"20\")\n",
    "\n",
    "try:\n",
    "    sheet3 = spreadsheet.worksheet(\"Feuille 3\")\n",
    "except gspread.exceptions.WorksheetNotFound:\n",
    "    sheet3 = spreadsheet.add_worksheet(title=\"Feuille 3\", rows=\"1000\", cols=\"20\")\n",
    "\n",
    "# Set the path to the chromedriver executable\n",
    "PATH = r\"C:\\Program Files (x86)\\chromedriver.exe\"\n",
    "s = Service(PATH)\n",
    "\n",
    "# Initialize the Chrome WebDriver\n",
    "driver = webdriver.Chrome(service=s)\n",
    "\n",
    "def extract_part(text):\n",
    "    parts = text.split('=')\n",
    "    result = f\"{parts[0]}={parts[1]}\"\n",
    "    return result\n",
    "\n",
    "# Initialize a list to store the new job links\n",
    "job_links = []\n",
    "\n",
    "# Load existing links\n",
    "links_processed = []\n",
    "try:\n",
    "    existing_df = get_as_dataframe(sheet2)\n",
    "    if not existing_df.empty:\n",
    "        links_processed = existing_df[\"Links\"].dropna().tolist()\n",
    "    print(f\"Loaded {len(links_processed)} existing links.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Open the webpage\n",
    "urle =\"https://fr.indeed.com/jobs?q=offre+emploi+remote&l=France&from=searchOnDesktopSerp&vjk=564057c05c230ab8\" \n",
    "driver.get(urle)\n",
    "country = extract_location(urle)\n",
    "time.sleep(1)\n",
    "\n",
    "verif = True\n",
    "i = 1\n",
    "while i <= 1 and verif:\n",
    "    job_container = driver.find_element(By.ID, \"mosaic-jobResults\")\n",
    "    jobs = job_container.find_elements(By.TAG_NAME, \"li\")\n",
    "    for job in jobs:\n",
    "        try:\n",
    "            job_link_element = job.find_element(By.CLASS_NAME, \"jcs-JobTitle\")\n",
    "            job_link = job_link_element.get_attribute(\"href\")\n",
    "            clean_link = extract_part(job_link)\n",
    "            if clean_link not in links_processed:\n",
    "                job_links.append(job_link)\n",
    "                links_processed.append(clean_link)\n",
    "            else:\n",
    "                print(f\"Link already processed: {clean_link}\")\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "        except Exception as e:\n",
    "            print('An error occurred:', str(e))\n",
    "    \n",
    "    try:\n",
    "        button = driver.find_element(By.CSS_SELECTOR, f'a[data-testid=\"pagination-page-{i + 1}\"]')\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView(true);\", button)\n",
    "        time.sleep(1)\n",
    "        button.click()\n",
    "        time.sleep(2)\n",
    "    except NoSuchElementException:\n",
    "        verif = False\n",
    "    except Exception as e:\n",
    "        print('An error occurred while clicking the pagination button:', str(e))\n",
    "        break\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "# Create DataFrames\n",
    "df_new_links = pd.DataFrame({'Links': job_links})\n",
    "df_all_links = pd.DataFrame({'Links': links_processed})\n",
    "\n",
    "# Save new links to Feuille 3\n",
    "try:\n",
    "    set_with_dataframe(sheet3, df_new_links)\n",
    "    print(f\"New links saved to 'Feuille 3'.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while saving new links to 'Feuille 3': {e}\")\n",
    "\n",
    "# Save all links (existing and new) to Feuille 2\n",
    "try:\n",
    "    existing_links_df = get_as_dataframe(sheet2)\n",
    "    combined_df = pd.concat([existing_links_df, df_all_links], ignore_index=True).drop_duplicates().reset_index(drop=True)\n",
    "    sheet2.clear()\n",
    "    set_with_dataframe(sheet2, combined_df)\n",
    "    print(f\"All links saved to 'Feuille 2'.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while saving all links to 'Feuille 2': {e}\")\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job information extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on est dans  0\n",
      "on est dans  1\n",
      "on est dans  2\n",
      "on est dans  3\n",
      "on est dans  4\n",
      "on est dans  5\n",
      "on est dans  6\n",
      "on est dans  7\n",
      "on est dans  8\n",
      "error dans ce lien Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\".jobsearch-JobInfoHeader-title\"}\n",
      "  (Session info: chrome=127.0.6533.120); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF61D07EEA2+31554]\n",
      "\t(No symbol) [0x00007FF61CFF7ED9]\n",
      "\t(No symbol) [0x00007FF61CEB872A]\n",
      "\t(No symbol) [0x00007FF61CF08434]\n",
      "\t(No symbol) [0x00007FF61CF0853C]\n",
      "\t(No symbol) [0x00007FF61CF4F6A7]\n",
      "\t(No symbol) [0x00007FF61CF2D06F]\n",
      "\t(No symbol) [0x00007FF61CF4C977]\n",
      "\t(No symbol) [0x00007FF61CF2CDD3]\n",
      "\t(No symbol) [0x00007FF61CEFA33B]\n",
      "\t(No symbol) [0x00007FF61CEFAED1]\n",
      "\tGetHandleVerifier [0x00007FF61D388B1D+3217341]\n",
      "\tGetHandleVerifier [0x00007FF61D3D5AE3+3532675]\n",
      "\tGetHandleVerifier [0x00007FF61D3CB0E0+3489152]\n",
      "\tGetHandleVerifier [0x00007FF61D12E776+750614]\n",
      "\t(No symbol) [0x00007FF61D00375F]\n",
      "\t(No symbol) [0x00007FF61CFFEB14]\n",
      "\t(No symbol) [0x00007FF61CFFECA2]\n",
      "\t(No symbol) [0x00007FF61CFEE16F]\n",
      "\tBaseThreadInitThunk [0x00007FFF4C20257D+29]\n",
      "\tRtlUserThreadStart [0x00007FFF4DBAAF28+40]\n",
      "\n",
      "on est dans  9\n",
      "on est dans  10\n",
      "on est dans  11\n",
      "on est dans  12\n",
      "on est dans  13\n",
      "on est dans  14\n",
      "Data saved to 'Feuille 3'.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import pandas as pd\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from gspread_dataframe import get_as_dataframe, set_with_dataframe\n",
    "\n",
    "# Define scope and credentials\n",
    "scope = ['https://www.googleapis.com/auth/spreadsheets', \"https://www.googleapis.com/auth/drive.file\", \"https://www.googleapis.com/auth/drive\"]\n",
    "Cred = ServiceAccountCredentials.from_json_keyfile_name('credentials.json', scope)\n",
    "client = gspread.authorize(Cred)\n",
    "\n",
    "# Open the existing Google Sheet and get \"Feuille 3\"\n",
    "spreadsheet = client.open('try')  # Replace with your Google Sheet name\n",
    "sheet = spreadsheet.worksheet('Feuille 3')\n",
    "\n",
    "# Read existing links from the sheet into a DataFrame\n",
    "existing_df = get_as_dataframe(sheet)\n",
    "\n",
    "# Extract the 'Links' column\n",
    "links = existing_df['Links']\n",
    "\n",
    "# Set the path to the chromedriver executable\n",
    "PATH = r\"C:\\Program Files (x86)\\chromedriver.exe\"\n",
    "s = Service(PATH)\n",
    "driver = webdriver.Chrome(service=s)\n",
    "\n",
    "# Initialize lists to store scraped data\n",
    "company_name = []\n",
    "company_link = []\n",
    "job_location = []\n",
    "salaire = []\n",
    "type = []\n",
    "horaire = []\n",
    "description = []\n",
    "job_title = []\n",
    "\n",
    "try:\n",
    "    for i, link in enumerate(links):\n",
    "        print(\"on est dans \", i)\n",
    "        # if i > 10 : \n",
    "        #     break\n",
    "        try:\n",
    "            driver.get(link)\n",
    "            time.sleep(2)\n",
    "            job_title.append(driver.find_element(By.CLASS_NAME, \"jobsearch-JobInfoHeader-title\").text)\n",
    "            company_link_container = driver.find_element(By.CSS_SELECTOR, 'div[data-testid=\"jobsearch-CompanyInfoContainer\"]')\n",
    "            company_link_element = company_link_container.find_element(By.TAG_NAME, \"a\")\n",
    "\n",
    "            try:\n",
    "                company_name.append(company_link_container.find_element(By.TAG_NAME, \"a\").text)\n",
    "            except NoSuchElementException:\n",
    "                company_name.append(\"Null\")\n",
    "            try:\n",
    "                company_link.append(company_link_element.get_attribute(\"href\"))\n",
    "            except NoSuchElementException:\n",
    "                company_link.append(\"Null\")\n",
    "            try:\n",
    "                job_location.append(driver.find_element(By.ID, \"jobLocationText\").text)\n",
    "            except NoSuchElementException:\n",
    "                job_location.append(\"Null\")\n",
    "            try:\n",
    "                salaire_element = driver.find_element(By.XPATH, '//*[@aria-label=\"Salaire\"]')\n",
    "                salaire.append(salaire_element.find_element(By.TAG_NAME, \"ul\").text)\n",
    "            except NoSuchElementException:\n",
    "                salaire.append(\"Null\")\n",
    "            try:\n",
    "                type_element = driver.find_element(By.XPATH, '//*[@aria-label=\"Type de poste\"]')\n",
    "                type.append(type_element.find_element(By.TAG_NAME, \"ul\").text)\n",
    "            except NoSuchElementException:\n",
    "                type.append(\"Null\")\n",
    "            try:\n",
    "                horaire_element = driver.find_element(By.XPATH, '//*[@aria-label=\"Temps de travail\"]')\n",
    "                horaire.append(horaire_element.find_element(By.TAG_NAME, \"ul\").text)\n",
    "            except NoSuchElementException:\n",
    "                horaire.append(\"Null\")\n",
    "            try:\n",
    "                description_element = driver.find_element(By.ID, 'jobDescriptionText')\n",
    "                description.append(description_element.text)\n",
    "            except NoSuchElementException:\n",
    "                description.append(\"Null\")\n",
    "        except Exception as e:\n",
    "            print(\"error dans ce lien\", e)\n",
    "except Exception as e:\n",
    "    print(\"Error in processing link\", i, \":\", e)\n",
    "\n",
    "# Ensure all lists have the same length\n",
    "max_length = max(len(job_title), len(description), len(job_location), len(salaire), len(type), len(horaire), len(company_link), len(company_name))\n",
    "\n",
    "# Fill shorter lists with \"Null\"\n",
    "def fill_list(lst, max_length):\n",
    "    return lst + [\"Null\"] * (max_length - len(lst))\n",
    "\n",
    "job_title = fill_list(job_title, max_length)\n",
    "description = fill_list(description, max_length)\n",
    "job_location = fill_list(job_location, max_length)\n",
    "salaire = fill_list(salaire, max_length)\n",
    "type = fill_list(type, max_length)\n",
    "horaire = fill_list(horaire, max_length)\n",
    "company_link = fill_list(company_link, max_length)\n",
    "company_name = fill_list(company_name, max_length)\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    \"Offre d'emploi\": job_title,\n",
    "    \"Description de l'emploi\": description,\n",
    "    'Localisation': job_location,\n",
    "    'Salaire': salaire,\n",
    "    'Type emploi': type,\n",
    "    'Horaire': horaire,\n",
    "    'lien entreprise': company_link,\n",
    "    'Nom entreprise': company_name\n",
    "})\n",
    "\n",
    "# Write the DataFrame to the existing Google Sheet (Feuille 3)\n",
    "try:\n",
    "    sheet.clear()\n",
    "    set_with_dataframe(sheet, data)\n",
    "    print(f\"Data saved to 'Feuille 3'.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while saving data to 'Feuille 3': {e}\")\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the existing Google Sheet\n",
    "spreadsheet = client.open('Indeed')  # Replace with your Google Sheet name\n",
    "sheet = spreadsheet.worksheet('Feuille 3')\n",
    "# Read data from the Google Sheet into a DataFrame\n",
    "df = get_as_dataframe(sheet)\n",
    "# Replace 'null' with pd.NA\n",
    "df.replace('null', pd.NA, inplace=True)\n",
    "# Drop rows where all elements are NaN\n",
    "df_cleaned = df.dropna(how='all')\n",
    "# Sort the DataFrame by the 'Nom entreprise' column\n",
    "df_sorted = df_cleaned.sort_values(by=\"Nom entreprise\")\n",
    "# Clear the existing sheet content\n",
    "sheet.clear()\n",
    "# Write the cleaned and sorted DataFrame back to the Google Sheet\n",
    "set_with_dataframe(sheet, df_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# company information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing link  0\n",
      "1768\n",
      "Element found: Taille de l'entreprise\n",
      "plus de\n",
      "10 000\n",
      "Size extracted: plus de\n",
      "10 000\n",
      "profit est 4 à 8 Mrd (EUR)\n",
      "le secteur est Production chimique\n",
      "Processing link  1\n",
      "année not founded\n",
      "Size element not found\n",
      "Processing link  2\n",
      "année not founded\n",
      "Size element not found\n",
      "le secteur est Médias et communication\n",
      "Processing link  3\n",
      "année not founded\n",
      "Size element not found\n",
      "le secteur est Ressources humaines et recrutement\n",
      "no load more\n",
      "Processing link  4\n",
      "année not founded\n",
      "Element found: Taille de l'entreprise\n",
      "51 à 200\n",
      "Size extracted: 51 à 200\n",
      "le secteur est Énergie et exploitation des ressources naturelles\n",
      "no load more\n",
      "Processing link  5\n",
      "1999\n",
      "Element found: Taille de l'entreprise\n",
      "plus de\n",
      "10 000\n",
      "Size extracted: plus de\n",
      "10 000\n",
      "profit est 850 M à 4 Mrd (EUR)\n",
      "le secteur est Pharmaceutique et biotechnologie\n",
      "Company Pennylane already processed. Filling with last extracted data...\n",
      "Processing link  7\n",
      "année not founded\n",
      "Element found: Taille de l'entreprise\n",
      "201 à 500\n",
      "Size extracted: 201 à 500\n",
      "profit est 20 à 85 M (EUR)\n",
      "le secteur est Informatique\n",
      "Processing link  8\n",
      "1965\n",
      "Element found: Taille de l'entreprise\n",
      "5 001 à 10 000\n",
      "Size extracted: 5 001 à 10 000\n",
      "le secteur est Industrie manufacturière\n",
      "no load more\n",
      "Processing link  9\n",
      "année not founded\n",
      "Size element not found\n",
      "le secteur est Informatique\n",
      "no load more\n",
      "Processing link  10\n",
      "1998\n",
      "Element found: Taille de l'entreprise\n",
      "1 001 à 5 000\n",
      "Size extracted: 1 001 à 5 000\n",
      "profit est 85 à 450 M (EUR)\n",
      "le secteur est Gestion comptable et fiscale\n",
      "Processing link  11\n",
      "année not founded\n",
      "Element found: Taille de l'entreprise\n",
      "51 à 200\n",
      "Size extracted: 51 à 200\n",
      "profit est 850 k à 4 M (EUR)\n",
      "le secteur est Services web\n",
      "Processing link  12\n",
      "année not founded\n",
      "Size element not found\n",
      "Processing link  13\n",
      "1943\n",
      "Element found: Taille de l'entreprise\n",
      "plus de\n",
      "10 000\n",
      "Size extracted: plus de\n",
      "10 000\n",
      "le secteur est Magasins de meubles et d'articles ménagers\n",
      "no load more\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.common.exceptions import NoSuchElementException, WebDriverException\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from gspread_dataframe import get_as_dataframe, set_with_dataframe\n",
    "\n",
    "# Define scope and credentials\n",
    "scope = ['https://www.googleapis.com/auth/spreadsheets', 'https://www.googleapis.com/auth/drive']\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_name('credentials.json', scope)\n",
    "client = gspread.authorize(creds)\n",
    "\n",
    "# Open the Google Sheet by name\n",
    "sheet_name = 'try'\n",
    "spreadsheet = client.open(sheet_name)\n",
    "\n",
    "# Access \"Feuille 3\" for input\n",
    "try:\n",
    "    sheet3 = spreadsheet.worksheet('Feuille 3')\n",
    "except gspread.exceptions.WorksheetNotFound:\n",
    "    raise Exception(\"Feuille 3 not found in the spreadsheet\")\n",
    "\n",
    "# Access \"Feuille 1\" for output\n",
    "try:\n",
    "    sheet1 = spreadsheet.worksheet('Feuille 1')\n",
    "except gspread.exceptions.WorksheetNotFound:\n",
    "    sheet1 = spreadsheet.add_worksheet(title='Feuille 1', rows=1000, cols=20)\n",
    "\n",
    "# Read data from \"Feuille 3\" into a DataFrame\n",
    "data = get_as_dataframe(sheet3)\n",
    "\n",
    "# Read existing data from \"Feuille 1\"\n",
    "existing_data = get_as_dataframe(sheet1)\n",
    "existing_data = existing_data.dropna(how='all')  # Drop any completely empty rows\n",
    "\n",
    "# Add 'Pays' column with pays\n",
    "data['Pays'] = country\n",
    "\n",
    "# Extract the desired columns\n",
    "links = data['lien entreprise']\n",
    "company_names = data['Nom entreprise']\n",
    "\n",
    "# List to keep track of processed company names\n",
    "check_name = []\n",
    "\n",
    "# Variables to hold the last extracted data\n",
    "last_extracted_data = {\n",
    "    \"Nom entreprise\": \"\",\n",
    "    \"Description de l'entreprise\": \"\",\n",
    "    'Site web': \"\",\n",
    "    'Secteur': \"\",\n",
    "    \"Taille de l'entreprise\": \"\",\n",
    "    'Siège social': \"\",\n",
    "    'Profit': \"\",\n",
    "    \"Année de fondation\": \"\"\n",
    "}\n",
    "\n",
    "# Initialize ChromeDriver\n",
    "PATH = r\"C:\\Program Files (x86)\\chromedriver.exe\"\n",
    "s = Service(PATH)\n",
    "driver = webdriver.Chrome(service=s)\n",
    "\n",
    "for i, link in enumerate(links):\n",
    "    # if i > 10:\n",
    "    #     break\n",
    "    company = company_names[i]\n",
    "\n",
    "    if company in check_name:\n",
    "        print(f\"Company {company} already processed. Filling with last extracted data...\")\n",
    "        # Fill the row with the last extracted data\n",
    "        data.loc[i, \"Description de l'entreprise\"] = last_extracted_data[\"Description de l'entreprise\"]\n",
    "        data.loc[i, 'Site web'] = last_extracted_data['Site web']\n",
    "        data.loc[i, 'Secteur'] = last_extracted_data['Secteur']\n",
    "        data.loc[i, \"Taille de l'entreprise\"] = last_extracted_data[\"Taille de l'entreprise\"]\n",
    "        data.loc[i, 'Siège social'] = last_extracted_data['Siège social']\n",
    "        data.loc[i, 'Profit'] = last_extracted_data['Profit']\n",
    "        data.loc[i, \"Année de fondation\"] = last_extracted_data[\"Année de fondation\"]\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        driver.get(link)\n",
    "    except WebDriverException:\n",
    "        print(f\"Invalid link: {link}. Skipping to the next one.\")\n",
    "        continue\n",
    "    print(\"Processing link \", i)\n",
    "    time.sleep(1)\n",
    "\n",
    "    company_info = {}\n",
    "\n",
    "    try:\n",
    "        company_info['name'] = driver.find_element(By.XPATH, '//*[@itemprop=\"name\"]').text\n",
    "    except NoSuchElementException:\n",
    "        print(f\"Could not find company name for link: {link}\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        founded_element = driver.find_element(By.XPATH, '//*[@data-testid=\"companyInfo-founded\"]')\n",
    "        company_info['founded'] = founded_element.find_element(By.CLASS_NAME, \"css-vjn8gb\").text\n",
    "        print(company_info['founded'])\n",
    "    except NoSuchElementException:\n",
    "        print(\"année not founded\")\n",
    "        company_info['founded'] = \"Null\"\n",
    "\n",
    "    try:\n",
    "        size_element = driver.find_element(By.XPATH, '//*[@data-testid=\"companyInfo-employee\"]')\n",
    "        print(\"Element found:\", size_element.text)  # Debugging\n",
    "        company_info['size'] = size_element.find_element(By.CLASS_NAME, \"css-1opgcmt\").text\n",
    "        print(\"Size extracted:\", company_info['size'])  # Debugging\n",
    "    except NoSuchElementException:\n",
    "        print(\"Size element not found\")  # Debugging\n",
    "        company_info['size'] = \"Null\"\n",
    "\n",
    "    try:\n",
    "        profit_element = driver.find_element(By.XPATH, '//*[@data-testid=\"companyInfo-revenue\"]')\n",
    "        company_info['profit'] = profit_element.find_element(By.CLASS_NAME, \"css-1opgcmt\").text\n",
    "        print(\"profit est\",company_info['profit'])\n",
    "    except NoSuchElementException:\n",
    "        company_info['profit'] = \"Null\"\n",
    "\n",
    "    try:\n",
    "        secteur_element = driver.find_element(By.XPATH, '//*[@data-testid=\"companyInfo-industry\"]')\n",
    "        company_info['secteur'] = secteur_element.find_element(By.CLASS_NAME, \"css-vjn8gb\").text\n",
    "        print(\"le secteur est\",company_info['secteur'])\n",
    "    except NoSuchElementException:\n",
    "        company_info['secteur'] = \"Null\"\n",
    "\n",
    "    try:\n",
    "        localisation_element = driver.find_element(By.XPATH, '//*[@data-testid=\"companyInfo-headquartersLocation\"]')\n",
    "        company_info['localisation'] = localisation_element.find_element(By.TAG_NAME, \"span\").text\n",
    "    except NoSuchElementException:\n",
    "        company_info['localisation'] = \"Null\"\n",
    "\n",
    "    try:\n",
    "        website_element = driver.find_element(By.XPATH, '//*[@data-testid=\"companyInfo-companyWebsite\"]')\n",
    "        company_info['website'] = website_element.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\")\n",
    "    except NoSuchElementException:\n",
    "        company_info['website'] = \"Null\"\n",
    "\n",
    "    try:\n",
    "        description_element = driver.find_element(By.XPATH, '//*[@data-testid=\"less-text\"]')\n",
    "        description_less = description_element.text\n",
    "        try:\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView(true);\", description_element)\n",
    "            time.sleep(1)\n",
    "            load_more = description_element.find_element(By.TAG_NAME, 'button')\n",
    "            load_more.click()\n",
    "        except:\n",
    "            print('no load more')\n",
    "        try:\n",
    "            description_more = driver.find_element(By.XPATH, '//*[@data-testid=\"more-text\"]').text\n",
    "            company_info['description'] = description_more\n",
    "        except NoSuchElementException:\n",
    "            print(\"no description more\")\n",
    "            company_info['description'] = description_less\n",
    "    except NoSuchElementException:\n",
    "        company_info['description'] = \"Null\"\n",
    "\n",
    "    # Store the scraped data in the last_extracted_data dictionary\n",
    "    last_extracted_data = {\n",
    "        \"Nom entreprise\": company_info['name'],\n",
    "        \"Description de l'entreprise\": company_info['description'],\n",
    "        'Site web': company_info['website'],\n",
    "        'Secteur': company_info['secteur'],\n",
    "        \"Taille de l'entreprise\": company_info['size'],\n",
    "        'Siège social': company_info['localisation'],\n",
    "        'Profit': company_info['profit'],\n",
    "        \"Année de fondation\": company_info['founded']\n",
    "    }\n",
    "\n",
    "    # Fill the row with the extracted data\n",
    "    data.loc[i, \"Description de l'entreprise\"] = last_extracted_data[\"Description de l'entreprise\"]\n",
    "    data.loc[i, 'Site web'] = last_extracted_data['Site web']\n",
    "    data.loc[i, 'Secteur'] = last_extracted_data['Secteur']\n",
    "    data.loc[i, \"Taille de l'entreprise\"] = last_extracted_data[\"Taille de l'entreprise\"]\n",
    "    data.loc[i, 'Siège social'] = last_extracted_data['Siège social']\n",
    "    data.loc[i, 'Profit'] = last_extracted_data['Profit']\n",
    "    data.loc[i, \"Année de fondation\"] = last_extracted_data[\"Année de fondation\"]\n",
    "\n",
    "    # Add the company name to check_name\n",
    "    check_name.append(company)\n",
    "\n",
    "\n",
    "# Créer une fonction pour mapper les secteurs aux catégories\n",
    "def map_to_category(sector):\n",
    "    if sector in [\n",
    "    'Informatique',\n",
    "    \"Services d'assistance informatique\",\n",
    "    'Services web',\n",
    "    'Matériel informatique',\n",
    "    'Sécurité informatique et des réseaux',\n",
    "    'Développement de logiciels',\n",
    "    'Technologies et services de l’information',\n",
    "    'Services et conseil en informatique',\n",
    "    'Télécommunications',\n",
    "    'Technologie, information et Internet',\n",
    "    'Produits de réseaux informatiques',\n",
    "    'Fabrication de produits informatiques et électroniques',\n",
    "    'Services de marketing',\n",
    "    'Jeux vidéo',\n",
    "    'Services de blockchain'\n",
    "]:\n",
    "        return 'IT'\n",
    "    elif sector in [\n",
    "        'Services bancaires', 'Activités de placement pour compte propre',\n",
    "        'Services financiers', 'Assurances', 'Comptabilité','Finance','Services bancaires et de prêts','Traitement des transactions financières',\n",
    "    ]:\n",
    "        return 'Finance'\n",
    "    elif sector in [\n",
    "    'Services de bien-être et installations sportives',\n",
    "    'Hôpitaux et services de santé',\n",
    "    'Soins de santé mentale',\n",
    "    'Fabrication d’équipements médicaux',\n",
    "    'Recherche en biotechnologie',\n",
    "    'Cosmétiques'\n",
    "]:\n",
    "        return 'Santé'\n",
    "    elif sector in [\n",
    "    'Pharmaceutique et biotechnologie',\n",
    "    'Programmes d’administration de l’éducation',\n",
    "    'Enseignement primaire et secondaire',\n",
    "    'Enseignement supérieur',\n",
    "    'Enseignement',\n",
    "    'Fournisseurs d’apprentissage en ligne'\n",
    "    'Enseignement et formation',\"Services d'enseignement et de formation\"\n",
    "]:\n",
    "        return 'Education'\n",
    "    else:\n",
    "        return 'Autre'\n",
    "\n",
    "# Appliquer la fonction à la colonne \"Secteur\" pour créer la nouvelle colonne \"Category\"\n",
    "data['Categorie'] = data['Secteur'].apply(map_to_category)\n",
    "# Merge new data with existing data\n",
    "final_data = pd.concat([existing_data, data], ignore_index=True)\n",
    "\n",
    "\n",
    "# \n",
    "# Appliquer la fonction à chaque ligne du DataFrame\n",
    "# combined_data['Categorie'] = combined_data['Secteur'].apply(map_to_category)\n",
    "\n",
    "desired_order = [\"Offre d'emploi\", \"Description de l'emploi\",'Localisation','Salaire','Type emploi','Horaire','lien entreprise','Nom entreprise',\"Description de l'entreprise\",'Site web','Secteur',\"Taille de l'entreprise\",'Siège social','Profit','Année de fondation','Categorie','Pays']\n",
    "\n",
    "# Reordering columns\n",
    "combined_data = final_data[desired_order]\n",
    "\n",
    "\n",
    "# Write the merged data back to \"Feuille 1\"\n",
    "set_with_dataframe(sheet1, combined_data)\n",
    "\n",
    "# Quit the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supp feuille 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le contenu de la feuille 3 a été supprimé avec succès.\n"
     ]
    }
   ],
   "source": [
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "# Define scope and credentials\n",
    "scope = ['https://www.googleapis.com/auth/spreadsheets', 'https://www.googleapis.com/auth/drive.file', 'https://www.googleapis.com/auth/drive']\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_name('credentials.json', scope)\n",
    "client = gspread.authorize(creds)\n",
    "\n",
    "# Open the Google Sheet by name\n",
    "sheet_name = 'try'\n",
    "spreadsheet = client.open(sheet_name)\n",
    "\n",
    "# Access \"Feuille 3\" for input\n",
    "try:\n",
    "    sheet3 = spreadsheet.worksheet('Feuille 3')\n",
    "except gspread.exceptions.WorksheetNotFound:\n",
    "    raise Exception(\"Feuille 3 not found in the spreadsheet\")\n",
    "\n",
    "# Clear the contents of \"Feuille 3\"\n",
    "sheet3.clear()\n",
    "\n",
    "print(\"Le contenu de la feuille 3 a été supprimé avec succès.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from gspread_dataframe import get_as_dataframe, set_with_dataframe\n",
    "import re\n",
    "\n",
    "# Define scope and credentials\n",
    "scope = ['https://www.googleapis.com/auth/spreadsheets', 'https://www.googleapis.com/auth/drive']\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_name('credentials.json', scope)\n",
    "client = gspread.authorize(creds)\n",
    "\n",
    "# Open the Google Sheet by name\n",
    "sheet_name = 'try'\n",
    "spreadsheet = client.open(sheet_name)\n",
    "\n",
    "# Access \"Feuille 1\" for output\n",
    "try:\n",
    "    sheet1 = spreadsheet.worksheet('Feuille 1')\n",
    "except gspread.exceptions.WorksheetNotFound:\n",
    "    sheet1 = spreadsheet.add_worksheet(title='Feuille 1', rows=1000, cols=20)\n",
    "\n",
    "# Read data from \"Feuille 3\" into a DataFrame\n",
    "data = get_as_dataframe(sheet1)\n",
    "\n",
    "def process_size(size_str):\n",
    "    if pd.isna(size_str) or size_str.strip() == \"\" or size_str.strip().lower() == \"null\":\n",
    "        return 0\n",
    "\n",
    "    # Remove non-breaking spaces (used for thousands separator) and other spaces\n",
    "    size_str = re.sub(r'\\s+', '', size_str.replace(\"\\u00a0\", \"\"))\n",
    "    \n",
    "    # Check for intervals like \"51 à 200\"\n",
    "    match = re.search(r'(\\d+)à(\\d+)', size_str)\n",
    "    if match:\n",
    "        return int(match.group(2))  # Return the upper bound of the interval\n",
    "    \n",
    "    # Check for cases like \"plus de 10 000\"\n",
    "    match = re.search(r'plusde(\\d+)', size_str.lower())\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    # Check for single numbers\n",
    "    match = re.search(r'(\\d+)', size_str)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    return 0\n",
    "\n",
    "# Add the new column \"taille num\" based on the \"Taille entreprise\" column\n",
    "data['taille num'] = data[\"Taille de l'entreprise\"].apply(process_size)\n",
    "\n",
    "# Write the updated DataFrame to \"Feuille 1\"\n",
    "set_with_dataframe(sheet1, data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sheets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
